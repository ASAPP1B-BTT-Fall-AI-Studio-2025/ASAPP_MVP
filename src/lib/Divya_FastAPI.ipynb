{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mhre_3K84b-K"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok langchain langchain-openai pydantic > /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN41S-5m4jXm",
        "outputId": "db974a7f-ce50-41d3-95d9-7fce4b6eeaa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://valarie-nongrooming-ilse.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"34Wm2vFpbiPv55mRQmj5oCrpy3w_3jrm47qTQyXWgQZg5rtz7\")\n",
        "\n",
        "# Set up ngrok tunnel to expose FastAPI app\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"Public URL:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUbDePM26jhG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gzip, json\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9MZoCb54t2d",
        "outputId": "0acc61fc-41c7-4fe1-cbd6-e0c4da0759cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded full dataset: 10042 conversations.\n",
            "FastAPI running at: NgrokTunnel: \"https://valarie-nongrooming-ilse.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [409]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2601:405:4401:2db0:7892:b8e:504b:a470:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2601:405:4401:2db0:7892:b8e:504b:a470:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš¡ Processed 10 conversations\n",
            "INFO:     2601:405:4401:2db0:7892:b8e:504b:a470:0 - \"GET /extract_from_dataset?n=10 HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš¡ Processed 10 conversations\n",
            "INFO:     2601:405:4401:2db0:7892:b8e:504b:a470:0 - \"GET /extract_from_dataset?n=10 HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [409]\n"
          ]
        }
      ],
      "source": [
        "# IMPORTS AND SETUP\n",
        "import re, asyncio, os, gzip, json, time\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "from fastapi import FastAPI, Body\n",
        "from pydantic import BaseModel\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain_openai import ChatOpenAI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "# CREATE APP + ADD MIDDLEWARE\n",
        "app = FastAPI(\n",
        "    title=\"ASAPP Unstructured Data Extractor\",\n",
        "    description=\"Extract emails, phone numbers, and order IDs using Regex and LLMs\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Add CORS middleware here (not later)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# CORE CLASSES AND FUNCTIONS\n",
        "\n",
        "# --- Regex extraction ---\n",
        "def regex_extract_fields(text: str) -> Dict[str, List[str]]:\n",
        "    email_match = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text)\n",
        "    phone_match = re.findall(r\"\\+?\\d[\\d\\s-]{8,}\\d\", text)\n",
        "    order_match = re.findall(r\"order[\\s#:]*\\d+\", text, re.IGNORECASE)\n",
        "    return {\n",
        "        \"emails\": list(set(email_match)),\n",
        "        \"phone_numbers\": list(set(phone_match)),\n",
        "        \"order_ids\": list(set(order_match))\n",
        "    }\n",
        "\n",
        "# --- LLM extractor ---\n",
        "class AsyncLLMExtractor:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "        schema = [\n",
        "            ResponseSchema(name=\"emails\", description=\"List of email addresses\"),\n",
        "            ResponseSchema(name=\"phone_numbers\", description=\"List of phone numbers\"),\n",
        "            ResponseSchema(name=\"order_ids\", description=\"List of order IDs\"),\n",
        "        ]\n",
        "        self.parser = StructuredOutputParser.from_response_schemas(schema)\n",
        "        self.instructions = self.parser.get_format_instructions()\n",
        "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        You are an intelligent assistant that extracts structured data from customer service conversations.\n",
        "        Identify all email addresses, phone numbers, and order IDs from the text below.\n",
        "        Return valid JSON with keys: \"emails\", \"phone_numbers\", and \"order_ids\".\n",
        "\n",
        "        Conversation:\n",
        "        {conversation}\n",
        "\n",
        "        {format_instructions}\n",
        "        \"\"\")\n",
        "\n",
        "    async def extract_async(self, text: str):\n",
        "        try:\n",
        "            prompt_input = self.prompt.format_prompt(\n",
        "                conversation=text,\n",
        "                format_instructions=self.instructions\n",
        "            )\n",
        "            output = await self.llm.agenerate([prompt_input.to_messages()])\n",
        "            content = output.generations[0][0].text\n",
        "            return self.parser.parse(content)\n",
        "        except Exception as e:\n",
        "            return {\"emails\": [], \"phone_numbers\": [], \"order_ids\": [], \"error\": str(e)}\n",
        "\n",
        "extractor = AsyncLLMExtractor()\n",
        "\n",
        "# ROUTES (DEFINE BEFORE RUNNING SERVER)\n",
        "#\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Welcome to the ASAPP Field Extraction API ðŸš€\"}\n",
        "\n",
        "@app.get(\"/extract_from_dataset\")\n",
        "async def extract_from_dataset(n: int = 10):\n",
        "    \"\"\"\n",
        "    Run regex+LLM extraction on first N conversations in the dataset.\n",
        "    Example: GET /extract_from_dataset?n=10\n",
        "    \"\"\"\n",
        "    subset = all_data[:n]\n",
        "    results = await process_conversations_async(subset, extractor, batch_size=5)\n",
        "    return {\"count\": len(results), \"results\": results}\n",
        "\n",
        "# DATASET LOADING + ASYNC PROCESSING\n",
        "def load_abcd_dataset():\n",
        "    with gzip.open(\"abcd_v1.1.json.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
        "        data_full = json.load(f)\n",
        "    all_data = data_full[\"train\"] + data_full[\"dev\"] + data_full[\"test\"]\n",
        "    print(f\"Loaded full dataset: {len(all_data)} conversations.\")\n",
        "    return all_data\n",
        "\n",
        "all_data = load_abcd_dataset()\n",
        "\n",
        "def convo_to_text(convo):\n",
        "    return \"\\n\".join([f\"{m[0].capitalize()}: {m[1]}\" for m in convo.get(\"original\", [])])\n",
        "\n",
        "async def process_conversations_async(conversations, extractor, batch_size=5):\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "    for i in tqdm(range(0, len(conversations), batch_size)):\n",
        "        batch = conversations[i:i+batch_size]\n",
        "        tasks = []\n",
        "        for convo in batch:\n",
        "            convo_text = convo_to_text(convo)\n",
        "            async def process_one(text):\n",
        "                regex_res = regex_extract_fields(text)\n",
        "                llm_res = await extractor.extract_async(text)\n",
        "                return {k: list(set(regex_res.get(k, []) + llm_res.get(k, []))) for k in [\"emails\", \"phone_numbers\", \"order_ids\"]}\n",
        "            tasks.append(process_one(convo_text))\n",
        "        batch_results = await asyncio.gather(*tasks)\n",
        "        results.extend(batch_results)\n",
        "    print(f\"âš¡ Processed {len(conversations)} conversations\")\n",
        "    return results\n",
        "\n",
        "# RUN FASTAPI SERVER\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from uvicorn import Config, Server\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"FastAPI running at:\", public_url)\n",
        "\n",
        "config = Config(app=app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "server = Server(config)\n",
        "await server.serve()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
