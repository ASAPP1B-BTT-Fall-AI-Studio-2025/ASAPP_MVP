# ABCD Dataset Integration Guide

## Overview

Extractify now supports the **ABCD (Airline Based Customer Conversation Dataset) v1.1**, which contains **8,034 customer service conversations**. The dataset is pre-loaded in your project as a compressed gzip file.

## Dataset Details

- **Format**: ABCD v1.1
- **Compression**: `abcd_v1.1.json.gz` (35 MB compressed, 116 MB uncompressed)
- **Total Conversations**: 8,034
- **Location**: `/sample-data/abcd_v1.1.json.gz`

## Dataset Structure

Each conversation in the ABCD dataset contains:

```json
{
  "convo_id": 3592,
  "scenario": {
    "personal": {
      "customer_name": "crystal minh",
      "email": "cminh730@email.com",
      "member_level": "bronze",
      "phone": "(977) 625-2661",
      "username": "cminh730"
    },
    "order": {
      "street_address": "...",
      "full_address": "...",
      "city": "...",
      "order_id": "...",
      "state": "...",
      "zip_code": "...",
      // ... more order details
    }
  },
  "original": [
    ["agent", "greeting text"],
    ["customer", "customer response"]
    // ... conversation turns
  ],
  "delexed": [
    {
      "speaker": "agent",
      "text": "how may i help you?",
      "turn_count": 1,
      "targets": [...],
      "candidates": [...]
    }
    // ... conversation turns (anonymized)
  ]
}
```

## Using the Dataset with Extractify

### Method 1: Decompress the Full Dataset

```bash
cd sample-data
gunzip -c abcd_v1.1.json.gz > abcd_v1.1.json
```

This creates a 116 MB JSON file containing all 8,034 conversations.

### Method 2: Use Pre-created Sample Files

We've created smaller sample files for easier testing:

- **`abcd_sample_10.json`** - 10 conversations (~290 KB)
- **`abcd_sample_50.json`** - 50 conversations (~1.3 MB)
- **`abcd_sample_100.json`** - 100 conversations (~2.7 MB)

These were automatically generated by the `create_samples.py` script.

### Method 3: Create Custom Sample Sizes

Edit and run `create_samples.py` to create different sample sizes:

```bash
python3 sample-data/create_samples.py
```

## Processing with Extractify

### Step 1: Start Both Servers

```bash
# Terminal 1: Start FastAPI backend
cd backend
python3 main.py

# Terminal 2: Start Next.js frontend
npm run dev
```

### Step 2: Upload the Dataset

1. Go to `http://localhost:3000`
2. Use the **"Upload File with Conversation"** section
3. Upload any of the sample files (e.g., `abcd_sample_10.json`)
4. Click the upload button or drag and drop the file

### Step 3: View Extraction Results

Extractify will:
- Detect the ABCD format automatically
- Extract all conversations from the file
- Process each conversation through the hybrid regex+LLM pipeline
- Display bulk processing status: "ðŸ“¦ Bulk Processing: X conversations processed"

### Step 4: Download Results

Click the **"Download Results"** button to export all extracted fields as a JSON file:

```json
{
  "fileName": "abcd_sample_10.json",
  "exportDate": "2025-12-06T00:30:00.123456",
  "totalRecords": 10,
  "results": [
    {
      "email": "extracted@email.com",
      "phone": "(555) 123-4567",
      "zipCode": "12345",
      "orderId": "ORDER123",
      "metadata": {
        "conversation_id": 3592,
        "fileName": "abcd_sample_10.json_convo_3592",
        "processedAt": "...",
        "textLength": ...,
        "extractionMethod": "hybrid",
        "regexResults": {...},
        "llmResults": {...}
      }
    }
    // ... more results
  ]
}
```

## What Gets Extracted

The extraction pipeline processes each conversation's text and extracts:

1. **Email Address** - Customer contact email
2. **Phone Number** - Customer phone number  
3. **ZIP Code** - Shipping/billing ZIP code
4. **Order ID** - Order reference number

Each extraction includes:
- **Regex Results**: Pattern-based extraction
- **LLM Results**: AI-powered extraction (uses GPT-4o-mini if available)
- **Final Result**: Best of both methods
- **Metadata**: Timestamps, file info, extraction confidence

## Sample Data Example

From conversation ID 3592:
- **Customer**: crystal minh
- **Email**: cminh730@email.com
- **Phone**: (977) 625-2661
- **ZIP Code**: 75227
- **Member Level**: bronze

## Performance Notes

- **10 conversations**: ~15-30 seconds
- **50 conversations**: ~1-2 minutes
- **100 conversations**: ~3-5 minutes
- **Full dataset (8,034)**: ~30-60 minutes (with LLM enabled)

Speed depends on:
- Whether LLM extraction is enabled
- Your OpenAI API connection
- System resources
- Network latency

## Troubleshooting

### File Not Found
Ensure the file is in the `sample-data` directory and use the correct filename.

### Processing Takes Too Long
- Start with smaller samples (10 or 50 conversations)
- Disable LLM extraction if API is slow
- Check `http://localhost:8000/health` to verify backend is running

### No Emails/Phones Extracted
Some conversations may not contain all fields. The extraction returns "NA" when fields are not found.

## API Endpoint

### POST /extract-bulk

**Request:**
```bash
curl -X POST http://localhost:8000/extract-bulk \
  -H "Content-Type: application/json" \
  -d '{
    "text": "[ABCD JSON data or JSONL]",
    "fileName": "abcd_sample_10.json"
  }'
```

**Response:**
```json
{
  "conversations": [
    {
      "email": "...",
      "phone": "...",
      "zipCode": "...",
      "orderId": "...",
      "metadata": {...}
    }
  ],
  "total": 10,
  "format": "abcd",
  "dataset": "ABCD v1.1"
}
```

## Next Steps

1. **Explore the Data**: Try different sample sizes to understand extraction quality
2. **Fine-tune Extraction**: Adjust regex patterns in `backend/main.py`
3. **Scale Up**: Process larger samples as you optimize performance
4. **Integrate Results**: Use extracted data for your downstream tasks

## References

- ABCD Dataset Paper: [Citation and link]
- Extractify README: See `../README.md`
- FastAPI Documentation: https://fastapi.tiangolo.com

---

**Dataset Location**: `/sample-data/abcd_v1.1.json.gz`  
**Decompressed**: `/sample-data/abcd_v1.1.json`  
**Samples**: `/sample-data/abcd_sample_*.json`
